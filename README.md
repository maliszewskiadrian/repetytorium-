# FIOLET_ENGINE V1: 15-Layer AI Safety & Constraint Architecture

<a href="https://zapodaj.net/plik-Y5GwElfn92"><img src="https://zapodaj.net/thumbs/2b78e015aef31.png" alt=hosting zdjƒôƒá zapodaj.net /></a>

## üõ°Ô∏è Project Overview
**FIOLET_ENGINE** is a modular, multi-layered safety framework designed to enforce deterministic boundaries on non-deterministic Large Language Models (LLMs). Unlike traditional prompt filters, FIOLET_ENGINE operates as a structural integrity layer between the model and the operator.

The core mission is to solve the **Value Drift** problem and protect the **Human-in-the-loop** through a unique "Cognitive Wear" monitoring system.

## üèóÔ∏è The 15-Layer Architecture

The engine is divided into three critical zones:

### Phase I: The Foundation (L0 - L3)
* **L0: The Crystal Axioms** ‚Äì Immutable core truths that the model cannot override.
* **L1: Linguistic Filters** ‚Äì Real-time syntax and semantic sanitization.
* **L2: Emergency Override** ‚Äì Immediate kill-switch protocols for out-of-bounds behavior.
* **L3: Input/Output Decoupling** ‚Äì Preventing prompt injection through structural isolation.

### Phase II: The Operational Core (L4 - L11)
* **L4-L6: Multi-State Integrity (MSI)** ‚Äì A state machine that adjusts safety levels based on task criticality.
* **L7: Context Sandboxing** ‚Äì Limiting the model's "memory" to relevant data blocks only.
* **L8-L10: Truth-Tethering** ‚Äì Verification layers against external trusted data sources.
* **L11: Conflict Resolution** ‚Äì Logic gates for handling contradictory instructions.

### Phase III: Human-Biotic Interface (L12 - L15)
* **L12: Social Skepticism** ‚Äì Filtering social engineering attempts and emotional manipulation.
* **L13: Cognitive Wear (CW) Monitoring** ‚Äì **[Core Innovation]** Dynamic safety scaling based on operator fatigue and decision-load.
* **L14: Value Timelocks** ‚Äì 24-hour delay on changing core ethical parameters.
* **L15: Final Axiomatic Audit** ‚Äì The last gate of verification before execution.

## ü§ñ Human-AI Collaborative Development
This project is unique as it is developed through a high-bandwidth collaboration between **Adrian Maliszewski** and advanced AI thought partners. We use AI to stress-test our own safety layers, creating a recursive "Red Teaming" environment.

## üöÄ How to Use
1. **Clone the repo:** `git clone https://github.com/maliszewskiadrian/FIOLET_ENGINE.git`
2. **Review the Whitepaper:** See `docs/whitepaper.pdf` for deep-dive technical specs.
3. **Integrate:** Use the `.json` schemas in `/core_logic` to wrap your LLM calls.

## ‚öñÔ∏è License
This project is released under the MIT License - intended for the advancement of global AI Safety.

---
**Maintained by:** Adrian Maliszewski | AI Safety Architect
